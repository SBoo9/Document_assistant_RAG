ğŸ§  Document Assistant RAG
A lightweight, local-first Retrieval-Augmented Generation (RAG) document assistant powered by FastAPI, LangChain, and FAISS. Upload PDF documents, process them into embeddings, and ask natural language questions with LLM-powered answersâ€”all while maintaining local control over your data.

ğŸš€ Features
âœ… Upload & process multiple PDF documents

ğŸ“„ Automatic text extraction, cleaning, and chunking

ğŸ§  Embedding via HuggingFace models (all-MiniLM-L6-v2)

ğŸ” FAISS vector store for efficient semantic retrieval

ğŸ—¨ï¸ Question answering via open-source LLMs (e.g., Mistral 7B)

ğŸ§± Modular FastAPI backend (LLM, chunking, RAG, etc.)

ğŸ§© LangChain integration for scalable pipelines

ğŸ’¾ Local persistence of vectors + metadata

ğŸ›ï¸ Optional: Streamlit UI frontend for real-time QA

ğŸ›  Tech Stack
FastAPI â€“ RESTful backend

LangChain â€“ RAG orchestration & scaling

FAISS â€“ Fast vector similarity search

HuggingFace â€“ Text embeddings

PyMuPDF / pdfminer.six â€“ PDF parsing

Mistral or HuggingFace models â€“ LLM QA

Streamlit â€“ Optional web frontend

ğŸ“¦ Installation
bash
Copy
Edit
git clone https://github.com/SBoo9/Document_assistang_RAG.git
cd Document_assistang_RAG
python -m venv venv
source venv/bin/activate  # or .\venv\Scripts\activate on Windows
pip install -r requirements.txt
â–¶ï¸ Run the FastAPI Backend
bash
Copy
Edit
uvicorn app.main:app --reload
Visit: http://localhost:8000/docs

ğŸŒ Streamlit Frontend (Optional)
bash
Copy
Edit
streamlit run frontend/app.py
ğŸ§© Coming Soon / Roadmap
âœ… Multi-document memory

âœ… Chunk/embedding persistence

ğŸ”„ Streaming answers

ğŸ”Œ Switchable LLM support

ğŸŒ HuggingFace/Mistral integration

â˜ï¸ S3 / DB support for production deployment